# üìúBasic Information

> - **Link**: [[2004.14224] Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning](https://arxiv.org/abs/2004.14224)
> - **comments**: /
> - **Tags**: #pre-training #KG 

# üñº Background & Key Problem

## 1. **Limited Structural Understanding in LLMs**

- The prevailing pretraining paradigm for LLMs focuses on learning contextual dependencies from large-scale text corpora, but this approach limits their ability to **comprehend structural knowledge**.
- This work proposes integrating KGs directly into the pretraining pipeline to address this limitation.

## 2. **Challenges in KG-Augmented LLMs**

- **During retrieval:**
    - LLMs rely on KGs to fetch external knowledge in downstream tasks.
    - Performance depends heavily on KG accuracy and completeness.
    - The process incurs significant computational costs.
- **During training:**
    - LLMs can be trained on textual data converted from KGs.
    - However, such models exhibit poor generalization, as they struggle to transfer knowledge effectively to other domains or tasks.

# üîå Methodology & Innovation

![[../pic/Pasted image 20251031150520.png]]

## 1. **Input Processing**

- The model takes **free-text** input.
- Each text is split into **morphemes**, which may correspond to **entities** in the KG.

## 2. **KG-Guided Entity Masking**

- 20% of token masks are **randomly sampled**.
- The remaining 80% are **guided by the KG structure**:
    - Morphemes appearing in the KG are categorized as **reachable** or **unreachable**, depending on whether they can be reached within _k_-hops from other entities in the same text.
    - Masked tokens are then sampled from these reachable entities.

## 3. **Task 1: KG-Guided Entity-Level Masked Language Modeling**

- Extends traditional masked language modeling by integrating KG-based entity relationships into the prediction objective.

## 4. **Task 2: Distractor-Suppressed Ranking**

- Negative samples are selected from entities not used in Task 1.
- The model learns to **lower the scores of semantically similar but incorrect entities** and **raise the scores of correct entities**, effectively suppressing distractors.

# üî¶ Personal Insights

## **Strengths**

- The study focuses on **entity-level representations**, leveraging KG connections to model relational structure.
- By masking similar but distinct or logically related entities, the model learns to **differentiate fine-grained semantic relations** within the KG.
- This pipeline enhances the LLM‚Äôs ability to understand not only linguistic logic but also the **semantic relationships among entities**.

## **Limitations**

- The approach relies on **complete and domain-aligned KGs**, which may not cover all entities in diverse text corpora.
- Since KGs encode reasoning chains rather than surface similarity, neighboring entities may be **logically connected but semantically dissimilar**, potentially confusing the model during training.

---

üìå **Personal Recommendation:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ